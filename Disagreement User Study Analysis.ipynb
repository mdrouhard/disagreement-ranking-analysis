{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Packages Used:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fisher Transformation\n",
    "* Background and Formula: https://en.wikipedia.org/wiki/Fisher_transformation\n",
    "* Implementation Reference: http://dept.stat.lsa.umich.edu/~kshedden/Python-Workshop/stats_calculations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fisherTransform(corr):\n",
    "    '''Given 1-row matrix of correlation coefficients, computes Fisher Transform matrix of same dimensions'''\n",
    "    \n",
    "    # Fisher transform all the correlation coefficients (equivalent to hyperbolic tangent)\n",
    "    F = np.arctanh(corr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO: Clean this up when we know what we want\n",
    "    \n",
    "    # sample size (n is number of columns, which is sample size)\n",
    "    #m, n = corr.shape\n",
    "    \n",
    "    # average correlation coefficient\n",
    "    #r = corr.mean(axis=1)\n",
    "\n",
    "    # StdDev and Error\n",
    "    #fStd = F.std(axis=1)\n",
    "    #fErr = (1/np.sqrt(n-3))\n",
    "    \n",
    "    #avgF = F.mean(axis=1)\n",
    "    \n",
    "    # Transform back to R Value\n",
    "    #convertedR = np.tanh(avgF)\n",
    "    \n",
    "    # 95% confidence intervals on the Fisher transform scale\n",
    "    #LCL = F - 2/np.sqrt(n-3)\n",
    "    #UCL = F + 2/np.sqrt(n-3)\n",
    "\n",
    "    # Convert the intervals back to the correlation scale\n",
    "    #LCL = (np.exp(2*LCL)-1) / (np.exp(2*LCL)+1)\n",
    "    #UCL = (np.exp(2*UCL)-1) / (np.exp(2*UCL)+1)\n",
    "    \n",
    "    # Coverage Probability\n",
    "    #CP = np.mean((LCL < r) & (r < UCL))\n",
    "    \n",
    "    #print ('\\n\\nAverage Fisher transformation:')\n",
    "    #print (avgF)\n",
    "    \n",
    "    #print ('\\nThe standard deviation of the Fisher transformed correlation coefficients is: ' + str(fStd))\n",
    "    #print ('\\nErr = 1/sqrt(n-3)=' + str(fErr))\n",
    "    \n",
    "    #print ('\\nConverted back to R Value:')\n",
    "    #print (convertedR)\n",
    "\n",
    "    # TODO: This does not seem right!!\n",
    "    #print ('The coverage probability is: ' + str(CP))\n",
    "    \n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avgFisherTransform(F):\n",
    "    '''Given 1-row Matrix of Fisher Transform Values, returns Average Fisher Transform'''\n",
    "    return F.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fisherStdDev(F):\n",
    "    '''Given 1-row Matrix of Fisher Transform Values, returns std dev of Fisher Transform'''\n",
    "    return F.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fisherStdErr(n):\n",
    "    '''Given sample size n, returns standard error for Fisher Transform'''\n",
    "    return (1/np.sqrt(n-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertedAvgR(avgF):\n",
    "    '''Given Average Fisher Transform, computes Average R Correlation Coefficient'''\n",
    "    return np.tanh(avgF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load data into numpy array and pandas dataframes:**\n",
    "* **datasetXX_nTreeRanks:** Ordered n-ary tree ranks for dataset XX (A, B, C, or D)\n",
    "* **datasetXX_absRanks:** Ordered absolute post-traversal ranks for dataset XX (A, B, C, or D)\n",
    "* **TODO:** At some point, update with Regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData = np.loadtxt(fname='disagreement-mturk-raw-ids-rankings-only.csv', delimiter=',', skiprows=1, \n",
    "                     dtype=np.dtype([('version', 'i4'), ('responseId', 'S25'), \n",
    "                                     ('rDA_1', 'f4'), ('rDA_2', 'f4'), ('rDA_3', 'f4'), ('rDA_4', 'f4'), ('rDA_5', 'f4'), \n",
    "                                     ('sDA_1', 'f4'), ('sDA_2', 'f4'), ('sDA_3', 'f4'), ('sDA_4', 'f4'), ('sDA_5', 'f4'), \n",
    "                                     ('rDB_1', 'f4'), ('rDB_2', 'f4'), ('rDB_3', 'f4'), ('rDB_4', 'f4'), ('rDB_5', 'f4'), \n",
    "                                     ('sDB_1', 'f4'), ('sDB_2', 'f4'), ('sDB_3', 'f4'), ('sDB_4', 'f4'), ('sDB_5', 'f4'), \n",
    "                                     ('rDC_1', 'f4'), ('rDC_2', 'f4'), ('rDC_3', 'f4'), ('rDC_4', 'f4'), ('rDC_5', 'f4'), \n",
    "                                     ('sDC_1', 'f4'), ('sDC_2', 'f4'), ('sDC_3', 'f4'), ('sDC_4', 'f4'), ('sDC_5', 'f4'), \n",
    "                                     ('rDD_1', 'f4'), ('rDD_2', 'f4'), ('rDD_3', 'f4'), ('rDD_4', 'f4'), ('rDD_5', 'f4'), \n",
    "                                     ('sDD_1', 'f4'), ('sDD_2', 'f4'), ('sDD_3', 'f4'), ('sDD_4', 'f4'), ('sDD_5', 'f4'), \n",
    "                                     ('mTurkCode', 'S25')]))\n",
    "\n",
    "# initialize n-ary tree ranking results with headers\n",
    "nTresults = [['datasetId', 'overallCoeff', 'overallFTrans', 'overallConvCoeff',\n",
    "             'group1Coeff', 'group1FTrans', 'group1ConvCoeff',\n",
    "             'group2Coeff', 'group2FTrans', 'group2ConvCoeff']]\n",
    "\n",
    "# full data frame\n",
    "df = pd.DataFrame(rawData)\n",
    "\n",
    "# Utils for all datasets\n",
    "overallDropCols = [50,51]\n",
    "groupDropCols = [25, 26]\n",
    "NtId = 50\n",
    "\n",
    "# Ground Truth Ranking Datasets\n",
    "datasetA_nTreeRanks = df.ix[df['version']==3].filter(['rDA_1', 'rDA_2', 'rDA_3', 'rDA_4', 'rDA_5']).T\n",
    "datasetA_absRanks = df.ix[df['version']==4].filter(['rDA_1', 'rDA_2', 'rDA_3', 'rDA_4', 'rDA_5']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset A\n",
    "datasetA_overall_ranks = df.filter(['rDA_1', 'rDA_2', 'rDA_3', 'rDA_4', 'rDA_5']).T\n",
    "datasetA_group1_ranks = df.ix[(df['version']==1) | (df['version']==3) | (df['version']==4)].filter(['rDA_1', 'rDA_2', 'rDA_3', 'rDA_4', 'rDA_5']).T\n",
    "datasetA_group2_ranks = df.ix[(df['version']==2) | (df['version']==3) | (df['version']==4)].filter(['rDA_1', 'rDA_2', 'rDA_3', 'rDA_4', 'rDA_5']).T\n",
    "\n",
    "datasetA_overall_sliders = df.filter(['sDA_1', 'sDA_2', 'sDA_3', 'sDA_4', 'sDA_5']).T\n",
    "datasetA_group1_sliders = df.ix[(df['version']==1) | (df['version']==3) | (df['version']==4)].filter(['sDA_1', 'sDA_2', 'sDA_3', 'sDA_4', 'sDA_5']).T\n",
    "datasetA_group2_sliders = df.ix[(df['version']==2) | (df['version']==3) | (df['version']==4)].filter(['sDA_1', 'sDA_2', 'sDA_3', 'sDA_4', 'sDA_5']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset B\n",
    "datasetB_overall_ranks = df.filter(['rDB_1', 'rDB_2', 'rDB_3', 'rDB_4', 'rDB_5']).T\n",
    "datasetB_group1_ranks = df.ix[(df['version']==1) | (df['version']==3) | (df['version']==4)].filter(['rDB_1', 'rDB_2', 'rDB_3', 'rDB_4', 'rDB_5']).T\n",
    "datasetB_group2_ranks = df.ix[(df['version']==2) | (df['version']==3) | (df['version']==4)].filter(['rDB_1', 'rDB_2', 'rDB_3', 'rDB_4', 'rDB_5']).T\n",
    "\n",
    "datasetB_overall_sliders = df.filter(['sDB_1', 'sDB_2', 'sDB_3', 'sDB_4', 'sDB_5']).T\n",
    "datasetB_group1_sliders = df.ix[(df['version']==1) | (df['version']==3) | (df['version']==4)].filter(['sDB_1', 'sDB_2', 'sDB_3', 'sDB_4', 'sDB_5']).T\n",
    "datasetB_group2_sliders = df.ix[(df['version']==2) | (df['version']==3) | (df['version']==4)].filter(['sDB_1', 'sDB_2', 'sDB_3', 'sDB_4', 'sDB_5']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset C\n",
    "datasetC_overall_ranks = df.filter(['rDC_1', 'rDC_2', 'rDC_3', 'rDC_4', 'rDC_5']).T\n",
    "datasetC_group1_ranks = df.ix[(df['version']==1) | (df['version']==3) | (df['version']==4)].filter(['rDC_1', 'rDC_2', 'rDC_3', 'rDC_4', 'rDC_5']).T\n",
    "datasetC_group2_ranks = df.ix[(df['version']==2) | (df['version']==3) | (df['version']==4)].filter(['rDC_1', 'rDC_2', 'rDC_3', 'rDC_4', 'rDC_5']).T\n",
    "\n",
    "datasetC_overall_sliders = df.filter(['sDC_1', 'sDC_2', 'sDC_3', 'sDC_4', 'sDC_5']).T\n",
    "datasetC_group1_sliders = df.ix[(df['version']==1) | (df['version']==3) | (df['version']==4)].filter(['sDC_1', 'sDC_2', 'sDC_3', 'sDC_4', 'sDC_5']).T\n",
    "datasetC_group2_sliders = df.ix[(df['version']==2) | (df['version']==3) | (df['version']==4)].filter(['sDC_1', 'sDC_2', 'sDC_3', 'sDC_4', 'sDC_5']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset D\n",
    "datasetD_overall_ranks = df.filter(['rDD_1', 'rDD_2', 'rDD_3', 'rDD_4', 'rDD_5']).T\n",
    "datasetD_group1_ranks = df.ix[(df['version']==1) | (df['version']==3) | (df['version']==4)].filter(['rDD_1', 'rDD_2', 'rDD_3', 'rDD_4', 'rDD_5']).T\n",
    "datasetD_group2_ranks = df.ix[(df['version']==2) | (df['version']==3) | (df['version']==4)].filter(['rDD_1', 'rDD_2', 'rDD_3', 'rDD_4', 'rDD_5']).T\n",
    "\n",
    "datasetD_overall_sliders = df.filter(['sDD_1', 'sDD_2', 'sDD_3', 'sDD_4', 'sDD_5']).T\n",
    "datasetD_group1_sliders = df.ix[(df['version']==1) | (df['version']==3) | (df['version']==4)].filter(['sDD_1', 'sDD_2', 'sDD_3', 'sDD_4', 'sDD_5']).T\n",
    "datasetD_group2_sliders = df.ix[(df['version']==2) | (df['version']==3) | (df['version']==4)].filter(['sDD_1', 'sDD_2', 'sDD_3', 'sDD_4', 'sDD_5']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Calculate Correlations and Store Results:**\n",
    "* Pearson's used for Rank data\n",
    "* Spearman's used for continuous (slider) data\n",
    "* TODO: Currently only applying n-ary tree ranking, but data is in place for absolute ranking also (indices for n-ary tree are hard-coded currently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeStats(df, dropCols, gtId, statsMethod):\n",
    "    '''Helper function for buildStats. Given dataframe, indices for columns to drop, and \n",
    "    index for the ground truth ranking id, and stats method, it returns tuple of coefficients'''\n",
    "\n",
    "    # all correlations\n",
    "    corr = df.corr(method=statsMethod, min_periods=5)\n",
    "    # correlations with n-ary tree ranking\n",
    "    #TODO: these indices might need to be updated if we add absolute ranking\n",
    "    nTOverall = corr[-2:-1].drop(corr.columns[dropCols],axis=1)\n",
    "    # average n-ary tree correlation\n",
    "    avgNtOverall = nTOverall.mean(axis=1)\n",
    "    avgR = avgNtOverall[gtId]\n",
    "    \n",
    "    # average F-transform of n-ary tree metric\n",
    "    fTransform = fisherTransform(nTOverall)\n",
    "    avgFvec = avgFisherTransform(fTransform)\n",
    "    avgF = avgFvec[gtId]\n",
    "    \n",
    "    # converted average correlation coeff\n",
    "    convAvgRvec = convertedAvgR(avgFvec)\n",
    "    convR = convAvgRvec[gtId]\n",
    "    \n",
    "    return avgR,avgF,convR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildStats(dataId, overall, group1, group2, statsMethod):\n",
    "    '''Given a dataset name, overall data set and dataset for each group, returns list of results for pearson method'''\n",
    "    \n",
    "    #initialize empty list of stats\n",
    "    stats=[dataId]\n",
    "    \n",
    "    # overall data\n",
    "    overallR, overallF, overallConvR = computeStats(overall, overallDropCols, NtId, statsMethod)\n",
    "    stats.append(overallR)\n",
    "    stats.append(overallF)\n",
    "    stats.append(overallConvR)\n",
    "    \n",
    "    # group 1 data\n",
    "    group1R, group1F, group1ConvR = computeStats(group1, groupDropCols, NtId, statsMethod)\n",
    "    stats.append(group1R)\n",
    "    stats.append(group1F)\n",
    "    stats.append(group1ConvR)\n",
    "    \n",
    "    # group 2 data\n",
    "    group2R, group2F, group2ConvR = computeStats(group2, groupDropCols, NtId, statsMethod)\n",
    "    stats.append(group2R)\n",
    "    stats.append(group2F)\n",
    "    stats.append(group2ConvR)\n",
    "    \n",
    "    \n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: RuntimeWarning: divide by zero encountered in arctanh\n"
     ]
    }
   ],
   "source": [
    "# Dataset A Stats\n",
    "\n",
    "# Dataset A Ranks \n",
    "# Pearson Stats\n",
    "aRanksPearson = buildStats('datasetA_ranks_pearson', datasetA_overall_ranks, \n",
    "                           datasetA_group1_ranks, datasetA_group2_ranks, 'pearson')\n",
    "nTresults.append(aRanksPearson)\n",
    "# Spearman Stats\n",
    "aRanksSpearman = buildStats('datasetA_ranks_spearman', datasetA_overall_ranks, \n",
    "                           datasetA_group1_ranks, datasetA_group2_ranks, 'spearman')\n",
    "nTresults.append(aRanksSpearman)\n",
    "\n",
    "# Dataset A Sliders\n",
    "# Pearson Stats\n",
    "aSlidersPearson = buildStats('datasetA_sliders_pearson', datasetA_overall_sliders, \n",
    "                           datasetA_group1_sliders, datasetA_group2_sliders, 'pearson')\n",
    "nTresults.append(aSlidersPearson)\n",
    "# Spearman Stats\n",
    "aSlidersSpearman = buildStats('datasetA_sliders_spearman', datasetA_overall_sliders, \n",
    "                           datasetA_group1_sliders, datasetA_group2_sliders, 'spearman')\n",
    "nTresults.append(aSlidersSpearman)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset B Stats\n",
    "\n",
    "# Dataset B Ranks \n",
    "# Pearson Stats\n",
    "aRanksPearson = buildStats('datasetB_ranks_pearson', datasetB_overall_ranks, \n",
    "                           datasetB_group1_ranks, datasetB_group2_ranks, 'pearson')\n",
    "nTresults.append(aRanksPearson)\n",
    "# Spearman Stats\n",
    "aRanksSpearman = buildStats('datasetB_ranks_spearman', datasetB_overall_ranks, \n",
    "                           datasetB_group1_ranks, datasetB_group2_ranks, 'spearman')\n",
    "nTresults.append(aRanksSpearman)\n",
    "\n",
    "# Dataset B Sliders\n",
    "# Pearson Stats\n",
    "aSlidersPearson = buildStats('datasetB_sliders_pearson', datasetB_overall_sliders, \n",
    "                           datasetB_group1_sliders, datasetB_group2_sliders, 'pearson')\n",
    "nTresults.append(aSlidersPearson)\n",
    "# Spearman Stats\n",
    "aSlidersSpearman = buildStats('datasetB_sliders_spearman', datasetB_overall_sliders, \n",
    "                           datasetB_group1_sliders, datasetB_group2_sliders, 'spearman')\n",
    "nTresults.append(aSlidersSpearman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: RuntimeWarning: divide by zero encountered in arctanh\n"
     ]
    }
   ],
   "source": [
    "# Dataset C Stats\n",
    "\n",
    "# Dataset C Ranks \n",
    "# Pearson Stats\n",
    "aRanksPearson = buildStats('datasetC_ranks_pearson', datasetC_overall_ranks, \n",
    "                           datasetC_group1_ranks, datasetC_group2_ranks, 'pearson')\n",
    "nTresults.append(aRanksPearson)\n",
    "# Spearman Stats\n",
    "aRanksSpearman = buildStats('datasetC_ranks_spearman', datasetC_overall_ranks, \n",
    "                           datasetC_group1_ranks, datasetC_group2_ranks, 'spearman')\n",
    "nTresults.append(aRanksSpearman)\n",
    "\n",
    "# Dataset C Sliders\n",
    "# Pearson Stats\n",
    "aSlidersPearson = buildStats('datasetC_sliders_pearson', datasetC_overall_sliders, \n",
    "                           datasetC_group1_sliders, datasetC_group2_sliders, 'pearson')\n",
    "nTresults.append(aSlidersPearson)\n",
    "# Spearman Stats\n",
    "aSlidersSpearman = buildStats('datasetC_sliders_spearman', datasetC_overall_sliders, \n",
    "                           datasetC_group1_sliders, datasetC_group2_sliders, 'spearman')\n",
    "nTresults.append(aSlidersSpearman)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset D Stats\n",
    "\n",
    "# Dataset D Ranks \n",
    "# Pearson Stats\n",
    "aRanksPearson = buildStats('datasetD_ranks_pearson', datasetD_overall_ranks, \n",
    "                           datasetD_group1_ranks, datasetD_group2_ranks, 'pearson')\n",
    "nTresults.append(aRanksPearson)\n",
    "# Spearman Stats\n",
    "aRanksSpearman = buildStats('datasetD_ranks_spearman', datasetD_overall_ranks, \n",
    "                           datasetD_group1_ranks, datasetD_group2_ranks, 'spearman')\n",
    "nTresults.append(aRanksSpearman)\n",
    "\n",
    "# Dataset D Sliders\n",
    "# Pearson Stats\n",
    "aSlidersPearson = buildStats('datasetD_sliders_pearson', datasetD_overall_sliders, \n",
    "                           datasetD_group1_sliders, datasetD_group2_sliders, 'pearson')\n",
    "nTresults.append(aSlidersPearson)\n",
    "# Spearman Stats\n",
    "aSlidersSpearman = buildStats('datasetD_sliders_spearman', datasetD_overall_sliders, \n",
    "                           datasetD_group1_sliders, datasetD_group2_sliders, 'spearman')\n",
    "nTresults.append(aSlidersSpearman)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Results Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['datasetId', 'overallCoeff', 'overallFTrans', 'overallConvCoeff', 'group1Coeff', 'group1FTrans', 'group1ConvCoeff', 'group2Coeff', 'group2FTrans', 'group2ConvCoeff'], ['datasetA_ranks_pearson', 0.74799999999999978, inf, 1.0, 0.77199999999999991, inf, 1.0, 0.72399999999999975, inf, 1.0], ['datasetA_ranks_spearman', 0.74799999999999978, inf, 1.0, 0.77199999999999991, inf, 1.0, 0.72399999999999975, inf, 1.0], ['datasetA_sliders_pearson', -0.78179147148934414, -1.6448972890727971, -0.92815408104589259, -0.82145959412093827, -1.7551421417030695, -0.94195795200550192, -0.73671405940798718, -1.5196190474474873, -0.90863124738989554], ['datasetA_sliders_spearman', -0.78109122024850941, -inf, -1.0, -0.81880566202519656, -inf, -1.0, -0.73823390004772904, -inf, -1.0], ['datasetB_ranks_pearson', 0.71408093305126752, 1.4491458991107316, 0.89552385956554437, 0.83719833530148591, 1.713880869784127, 0.9371219655998464, 0.5909635308010488, 1.1844109284373376, 0.82883741492721485], ['datasetB_ranks_spearman', 0.71408093305126752, 1.4491458991107316, 0.89552385956554437, 0.83719833530148591, 1.713880869784127, 0.9371219655998464, 0.5909635308010488, 1.1844109284373376, 0.82883741492721485], ['datasetB_sliders_pearson', -0.71278448630325475, -1.4725034883712969, -0.90005394597975064, -0.86539182000454307, -1.7245021633960809, -0.93840286732743128, -0.5601771526019661, -1.2205048133465137, -0.83980302282785602], ['datasetB_sliders_spearman', -0.70812513302233326, -1.4262977626141566, -0.89090538762284599, -0.86649928618373795, -1.7530651014857006, -0.94172337936698292, -0.54975097986092814, -1.0995304237426138, -0.80033028593633193], ['datasetC_ranks_pearson', 0.72126278151586343, 1.3712527630182632, 0.87897738401219783, 0.81462681155561245, 1.5192983086469727, 0.90857529768532963, 0.62789875147611451, 1.2232072173895552, 0.84059769931475048], ['datasetC_ranks_spearman', 0.72126278151586343, 1.3712527630182632, 0.87897738401219783, 0.81462681155561245, 1.5192983086469727, 0.90857529768532963, 0.62789875147611451, 1.2232072173895552, 0.84059769931475048], ['datasetC_sliders_pearson', -0.70897308948446114, -1.3020102838487506, -0.86223977747299674, -0.84223567725126713, -1.5405779084631614, -0.91221742771144809, -0.57571050171765537, -1.0634426592343404, -0.78697791649750626], ['datasetC_sliders_spearman', -0.70068350103688559, -inf, -1.0, -0.82043653148313478, -1.5374117896732498, -0.91168442739867472, -0.58093047059063618, -inf, -1.0], ['datasetD_ranks_pearson', 0.60224929267398564, 1.1702490015867899, 0.82435197774026903, 0.74896419702216266, 1.6237535362734561, 0.92516685885136074, 0.45553438832580845, 0.71674446690012383, 0.61488870010732777], ['datasetD_ranks_spearman', 0.60224929267398564, 1.1702490015867899, 0.82435197774026903, 0.74896419702216266, 1.6237535362734561, 0.92516685885136074, 0.45553438832580845, 0.71674446690012383, 0.61488870010732777], ['datasetD_sliders_pearson', -0.52987214941360183, -1.0170581542271306, -0.7686655894223221, -0.71892427880560872, -1.4489662893053568, -0.8954882844537817, -0.34082002002159478, -0.58515001914890474, -0.52639848955517865], ['datasetD_sliders_spearman', -0.52280734333078216, -1.0146550709862656, -0.76768054254616747, -0.70728538549406328, -1.4658668081124189, -0.89878606628979862, -0.33832930116750093, -0.56344333386011303, -0.5105277742233798]]\n"
     ]
    }
   ],
   "source": [
    "print(nTresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Save Results as CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputDf = pd.DataFrame(nTresults)\n",
    "outputDf.to_csv('nTreeStats.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
